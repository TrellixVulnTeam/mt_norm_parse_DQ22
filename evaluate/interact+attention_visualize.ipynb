{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline  \n",
    "import sys\n",
    "sys.path.insert(0,\"..\")\n",
    "from predict.prediction_batch import greedy_decode_batch\n",
    "from predict.prediction_string import decode_seq_str, decode_interacively\n",
    "import pdb\n",
    "from model.loss import LossCompute\n",
    "import os\n",
    "from io_.info_print import printing\n",
    "from model.seq2seq import LexNormalizer\n",
    "from model.generator import Generator\n",
    "from evaluate.interact import interact\n",
    "MAX_LEN = 20\n",
    "script_dir = \".\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_folder_starts_with = \"f2f2-batchXdropout_char0.1-to_char_src-1_dir_sent-10_batch_size-model_18_aa04\"\n",
    "#model_folder_starts_with = \"f178-DROPOUT_EVEN_INCREASE-0.1-to_sent+word+bridge_out-model_3_046c\"\n",
    "#model_folder_starts_with = \"8ce6b-extend_ep-get_True-attention_simplifiedXauxXdropout0.1_scale_aux-True_aux-0.1do_char_dec-True_char_src_atten-model_14_ad6c\"\n",
    "#model_folder_starts_with = \"a5c77\"\n",
    "#model_folder_starts_with = \"fef8-new_data-batchXdropout_char0-to_char_src-1_dir_sent-10_batch_size-model_2_51a5\"\n",
    "#model_folder_starts_with = \"e3900-liu-attention+unrolling0.2-to_char_src-1_dir_sent-15_batch-dir_word_src_1-unrolling_word_True-char_src_attention_False-model_2_14c2\"\n",
    "#model_folder_starts_with = \"e3900-liu-attention+unrolling0.2-to_char_src-1_dir_sent-15_batch-dir_word_src_1-unrolling_word_True-char_src_attention_True-model_3_1a3\"\n",
    "#model_folder_starts_with =\"8ce6b\"\n",
    "#model_folder_starts_with = \"8e628\"\n",
    "#model_folder_starts_with = \"84736\"\n",
    "#model_folder_starts_with = \"84736-84736-SENT_context-get_True-attention_simplifiedXdrop_out_charXdir_word1dir_word-False_aux-0do_char_dec-True_char_src_atten-model_10_b59b\"\n",
    "#model_folder_starts_with = \"84736-84736-SENT_context-get_True-attention_simplifiedXdrop_out_charXdir_word1dir_word-False_aux-0do_char_dec-False_char_src_atten-model_9_bb70\"\n",
    "model_folder_starts_with = \"b9e49-aux_report+dense+ponderatipn+no_bucketing100-dense_dim_auxilliary0.001weight_binary_loss0.2-to_char_src-1_dir_sent-10_batch-dir_word_src_1-unrolling_word_True-char_src_attention_False-model_9_0f05\"\n",
    "#model_folder_starts_with = \"97068_rioc-b91d7-aux-again-biggerREPLICATE-replicate1-2dir_word-None_aux-model_19_fb56\"\n",
    "#model_folder_starts_with = \"97079_rioc-ef365-ATTbest-scale-2-True-25dir_word_encoder-all_context-att2-model_1_cade\"\n",
    "#model_folder_starts_with = \"97077_rioc-ee386-REP_-replicate1-1dir-scale_1-model_15_f19e\"\n",
    "#model_folder_starts_with = \"97147_rioc-6ed16-ATTCONTEXT-with_att-dir_2-X-dropout_bridge0.1-context_word-model_4_76a1\"\n",
    "#model_folder_starts_with = \"97147_rioc-6ed16-ATTCONTEXT-with_att-dir_2-X-dropout_bridge0.1-context_sent-model_2_7d92\"\n",
    "#model_folder_starts_with = \"97147_rioc-6ed16-ATTCONTEXT-with_att-dir_2-X-dropout_bridge0.1-context_word-model_4_76a1\"\n",
    "#model_folder_starts_with = \"97184_rioc-0cc46-ATTstandart_passing-with_att-dir_2-X-context_all-stable_decoding_state-init_context_decoder_False-model_4_2ffa\"\n",
    "#model_folder_starts_with = \"97221_rioc-3e4f2-ATTattentionXauxXcont-big-teacher-with_att-dir_2-X-context_word-stable_decod-init_con_False-teacher_forceprop10_True-model_4_c551\"\n",
    "#model_folder_starts_with = \"97221_rioc-3e4f2-ATTattentionXauxXcont-big-teacher-with_att-dir_2-X-context_all-stable_decod-init_con_False-teacher_forceprop10_True-model_2_7673\"\n",
    "#model_folder_starts_with = \"97555_rioc--ATTword_vs_char-scale2-sha_context_all-auxnorm_not_norm_True-word_de_False-model_6_c253-folder\"\n",
    "#model_folder_starts_with = \"97555_rioc--ATTword_vs_char-scale2-sha_context_all-auxnorm_not_norm_True-word_de_True-model_5_72ee-folder\"\n",
    "model_folder_starts_with = \"97184_rioc-0cc46-ATTstandart_passing-with_att-dir_2-X-context_all-stable_decoding_state-init_context_decoder_False-model_4_2ffa\"\n",
    "model_folder_starts_with = \"97679_rioc--ATTmodel_10-model_10_b22f-folder\"\n",
    "model_folder_starts_with = \"4e128-WARMUP-unrolling-False0-model_1-model_1_1660-folder\"\n",
    "model_folder_starts_with = \"97734_rioc--DEBUG-2LSMT-2dense0-model_3-model_3_88a4-folder\"\n",
    "#model_folder_starts_with = \"97734_rioc--DEBUG-2LSMT-2dense0-model_1-model_1_413f-folder\"\n",
    "#model_folder_starts_with = \"97734_rioc--DEBUG-2LSMT-2dense0-model_2-model_2_7a98\"\n",
    "model_folder_starts_with = \"97793_rioc--DEBUG_NO_LOSS_PADDING-2LSMT-2dense0-model_1-model_1_b872-folder\"\n",
    "model_folder_starts_with = \"97823_rioc--DEBUG_NO_LOSS_PADDING-LEAKY-2LSMT-2dense-5DROPOUT00-model_1-model_1_60d8-folder\"\n",
    "\n",
    "\n",
    "# model word context char decode no attn \n",
    "#model_folder_starts_with = \"97942_rioc--DEBUG_NO_LOSS_PADDING-0-model_1-model_1_767d-folder\"\n",
    "model_folder_starts_with = \"98349_rioc--DEBUG_NO_LOSS_PADDING-0-model_1-model_1_6249-folder\"\n",
    "model_folder_starts_with = \"98759_rioc--DEBUG_NO_LOSS_PADDING-0-model_1-model_1_b108-folder\"\n",
    "model_folder_starts_with = \"99428_rioc--DEBUG_NO_LOSS_PADDING-0-model_1-model_1_8fb8-folder\"\n",
    "#model_folder_starts_with = \"99428_rioc--DEBUG_NO_LOSS_PADDING-0-model_1-model_1_8fb8-start_3_ep-X1-train_longer\"\n",
    "model_folder_starts_with = \"99428_rioc--DEBUG_NO_LOSS_PADDING-0-model_1-model_1_8fb8-folder\"\n",
    "#\"98759_rioc--DEBUG_NO_LOSS_PADDING-0-model_1-model_1_b108-folder\"\n",
    "model_folder_starts_with = \"99733_rioc--DEBUG_NO_LOSS_PADDING-0-model_3-model_3_ccda-folder\"\n",
    "model_folder_starts_with =  \"99731_rioc--DEBUG_NO_LOSS_PADDING-0-model_1-model_1_6cf0-folder\"\n",
    "#model_folder_starts_with = \"99733_rioc--DEBUG_NO_LOSS_PADDING-0-model_4-model_4_5ce2-folder\"\n",
    "model_folder_starts_with = \"100147_rioc--DEBUG_NO_LOSS_PADDING-0-model_1-model_1_8b89-folder\"\n",
    "# fake words only char2char\n",
    "#model_folder_starts_with = \"EU\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['100147_rioc--DEBUG_NO_LOSS_PADDING-0-model_1-model_1_8b89-folder']\n"
     ]
    }
   ],
   "source": [
    "# list all models folder that starts with model_folder_starts_with\n",
    "list_all_dir = os.listdir(\"../checkpoints/\")\n",
    "list_ = [dir_ for dir_ in list_all_dir if dir_.startswith(model_folder_starts_with) and not dir_.endswith(\"log\") and not dir_.endswith(\"summary\")]\n",
    "print(list_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model to interact with \n",
    "\n",
    "#### Former models : trained on Liu only , source added as concatanation of word and sentence level as h_0 of decoder , batch size = 10 , small drop out , \n",
    "\n",
    "- f2f2-batchXdropout_char0.1-to_char_src-1_dir_sent-10_batch_size-model_18_aa04 : very bad at interacting (NB ; pb of eval)\n",
    "- f178 also good model trained on liu only \n",
    "- 8e628 : attention ; no aux (no bucket , get_batch False ) :  - attention degrades abit the results (still feedin char embedding also)\n",
    "- e390 + same 24f94 goo : \n",
    "    - best is e3900-liu-attention+unrolling0.2-to_char_src-1_dir_sent-15_batch-dir_word_src_1-unrolling_word_True-char_src_attention_False-model_2_14c2 \n",
    "    - same with attentin but lame : e3900-liu-attention+unrolling0.2-to_char_src-1_dir_sent-15_batch-dir_word_src_1-unrolling_word_True-char_src_attention_True-model_3_1a3f (have to do some code for reloading and visulizing !)\n",
    "\n",
    "#### New data \n",
    "- fef8_new_data : \n",
    "- mixed data+ aix test :  8d9a0 + b9e49 to compare bucketing impact at train time \n",
    "    - /Users/bemuller/Documents/Work/INRIA/dev/mt_norm_parse/reports/aux_report+dense+ponderatipn+bucketing-last-report-data.html or file:///Users/bemuller/Documents/Work/INRIA/dev/mt_norm_parse/reports/aux_report+dense+ponderatipn+bucketing-last-report-norm.html\n",
    "    - /Users/bemuller/Documents/Work/INRIA/dev/mt_norm_parse/reports/aux_report+dense+ponderatipn+no_bucketing-last-report-data.html or file:///Users/bemuller/Documents/Work/INRIA/dev/mt_norm_parse/reports/8d9a0-auxiililary_true_false.html\n",
    "    \n",
    "    \n",
    "- Auxilliary tuning : best model : b9e49-aux_report+dense+ponderatipn+no_bucketing100-dense_dim_auxilliary0.001weight_binary_loss0.2-to_char_src-1_dir_sent-10_batch-dir_word_src_1-unrolling_word_True-char_src_attention_False-model_9_0f05-folderfile:///Users/bemuller/Documents/Work/INRIA/dev/mt_norm_parse/reports/aux_report+dense+ponderatipn+no_bucketing-last-report-enriched-auxilliary_task_norm_not_norm-dense_dim_view.html\n",
    "-\n",
    "\n",
    "#### Last two ablations with extending epoch + mix data + ablation on auxilliary task ponderation + attention or not  \n",
    "\n",
    "- 8ce6b-extend_ep... \n",
    "    - super lame attention model (from ablation with all context)\n",
    "    - the other one a bit better   \n",
    "- a5c77 a bit better but still : no attention much better : cf. plot \n",
    "\n",
    "#### Smaller model + liu only \n",
    "- 84736 smaller model: still waiting for models : attention not helping \n",
    "\n",
    "NB : attention makes training 10 times slower  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!open /Users/bemuller/Documents/Work/INRIA/dev/mt_norm_parse/reports/ablation_DROPOUT_analysis.html # f178\n",
    "#!open /Users/bemuller/Documents/Work/INRIA/dev/mt_norm_parse/reports/ablation_DROPOUT_analysis_norm_view.html #f178\n",
    "#!open /Users/bemuller/Documents/Work/INRIA/dev/mt_norm_parse/reports/ablation_DROPOUT_analysis_2.html # f178 + aaad \n",
    "\n",
    "#!open /Users/bemuller/Documents/Work/INRIA/dev/mt_norm_parse/reports/f2f2-batchXdropout_char-summary-norm_view.html\n",
    "#!open /Users/bemuller/Documents/Work/INRIA/dev/mt_norm_parse/reports/f2f2-batchXdropout_char-summary-data_view.html\n",
    "# -- \n",
    "#!open /Users/bemuller/Documents/Work/INRIA/dev/mt_norm_parse/reports/f2f2_best+01880_reproduction-VAL_TRUE.html\n",
    "##file:///Users/bemuller/Documents/Work/INRIA/dev/mt_norm_parse/reports/f2f2-iterate+new_data-norm_view.html (different results with below ? get_batch due ?? )\n",
    "##/Users/bemuller/Documents/Work/INRIA/dev/mt_norm_parse/reports/f2f2-batchXdropout_char-summary-norm_view.html \n",
    "## diffent from file:///Users/bemuller/Documents/Work/INRIA/dev/mt_norm_parse/reports/f2f2_best+01880_reproduction-VAL_TRUE.html val True \n",
    "#--\n",
    "\n",
    "#!open /Users/bemuller/Documents/Work/INRIA/dev/mt_norm_parse/reports/liu-attention+unrolling-more-param.html #e390\n",
    "#!open file:///Users/bemuller/Documents/Work/INRIA/dev/mt_norm_parse/reports/e390_best+24f9d-VAL_FALSE.html (e390 with Vale False 10 points above!! )\n",
    "\n",
    "#!open /Users/bemuller/Documents/Work/INRIA/dev/mt_norm_parse/reports/fef8_new_data.html (very good ?)\n",
    "\n",
    "#!open /Users/bemuller/Documents/Work/INRIA/dev/mt_norm_parse/reports/extend_ep-SENT_context-get_True-attention_simplifiedXauxXdropout-last+bucket_False_eval-get_batch_False.html\n",
    "\n",
    "#!open /Users/bemuller/Documents/Work/INRIA/dev/mt_norm_parse/reports/8ce6b-extend_ep-get_True-attention_simplifiedXauxXdropout.html\n",
    "\n",
    "#!open /Users/bemuller/Documents/Work/INRIA/dev/mt_norm_parse/reports/8e628-no_bucketing-get_batch_False-train-attention-last+bucket_False_eval-get_batch_False-report.json.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "--------------------------------Interatcing with new model--------------------------------\n",
      " 100147_rioc--DEBUG_NO_LOSS_PADDING-0-model_1-model_1_8b89 \n",
      "\n",
      "\n",
      "Loading dictionary from ./../checkpoints/100147_rioc--DEBUG_NO_LOSS_PADDING-0-model_1-model_1_8b89-folder/dictionaries \n",
      "WARNING : checkpoint_dir as indicated in args.json is not found, lt checkpoint_dir /\n",
      "Loading model with argument {'lr_policy': 'lr_constant', 'hidden_size_sent_encoder': 200, 'voc_size': 517, 'auxilliary_arch': {'dense_dim_auxilliary_pos_2': 0, 'weight_binary_loss': 1, 'auxilliary_task_norm_not_norm': False, 'auxilliary_task_norm_not_norm-dense_dim_2': None, 'auxilliary_task_norm_not_norm-dense_dim': None, 'dense_dim_auxilliary_pos': 0, 'auxilliary_task_pos': False}, 'char_embedding_dim': 100, 'extend_vocab_with_test': True, 'symbolic_root': True, 'shared_context': 'all', 'hidden_size_encoder': 200, 'proportion_pred_train': None, 'output_dim': 200, 'word_voc_input_size': 20001, 'batch_size': 500, 'lr': 0.001, 'ponderation_normalize_loss': 1, 'encoder_arch': {'cell_word': 'LSTM', 'word_embedding_projected_dim': None, 'word_embed': False, 'n_layers_word_encoder': 1, 'word_embed_init': None, 'dropout_sent_encoder_cell': 0.5, 'word_embedding_dim': 0, 'dir_word_encoder': 2, 'n_layers_sent_cell': 2, 'dir_sent_encoder': 2, 'drop_out_sent_encoder_out': 0.0, 'cell_sentence': 'LSTM', 'drop_out_word_encoder_out': 0.5, 'dropout_word_encoder_cell': 0.3}, 'hidden_size_decoder': 100, 'symbolic_end': True, 'word_voc_output_size': None, 'tasks_schedule_policy': None, 'tasks': ['normalize'], 'weight_binary_loss': 0, 'n_trainable_parameters': 1021917, 'gradient_clipping': 1, 'decoder_arch': {'drop_out_char_embedding_decoder': 0.1, 'cell_sentence': 'none', 'cell_word': 'LSTM', 'word_decoding': False, 'teacher_force': True, 'char_src_attention': True, 'drop_out_word_decoder_cell': 0.3, 'drop_out_bridge': 0.5, 'dense_dim_word_pred': None, 'char_decoding': True, 'activation_char_decoder': 'nn.LeakyReLU', 'dense_dim_word_pred_2': None, 'unrolling_word': True, 'dir_word': 'uni', 'dense_dim_word_pred_3': None, 'stable_decoding_state': False, 'activation_word_decoder': 'nn.LeakyReLU', 'init_context_decoder': True}, 'weight_pos_loss': 0}\n",
      "Model arguments are {'info_checkpoint': {'proportion_pred_train': None, 'tasks_schedule_policy': None, 'n_epochs': 46, 'other': {'ponderation_normalize_loss': 1, 'average_per_epoch(min)': '2.72', 'optim_strategy': 'lr_constant', 'seed(np/torch)': [123, 123], 'data': 'dev', 'extend_n_batch': 2, 'time_training(min)': '138.63', 'error_curves_details': None, 'error_curves': '/scratch/bemuller/mt_norm_parse/env/../checkpoints/100147_rioc--DEBUG_NO_LOSS_PADDING-0-model_1-model_1_8b89-folder/100147_rioc--DEBUG_NO_LOSS_PADDING-0-model_1-model_1_8b89-last-plo-seq.png', 'weight_binary_loss': 0, 'weight_pos_loss': 0, 'lr': 0.001, 'loss': 4.4133834295620706e-05}, 'teacher_force': True, 'train_data_path': '/scratch/bemuller/mt_norm_parse/env/.././data/MTNT/train/train.en-fr.conll', 'batch_size': 500, 'git_id': 'abe89a87719fb12a3a9ec68c369e77740ef47edd', 'dev_data_path': '/scratch/bemuller/mt_norm_parse/env/.././data/MTNT/valid/valid.en-fr.conll', 'gradient_clipping': 1}, 'hyperparameters': {'lr_policy': 'lr_constant', 'hidden_size_sent_encoder': 200, 'voc_size': 517, 'auxilliary_arch': {'dense_dim_auxilliary_pos_2': 0, 'weight_binary_loss': 1, 'auxilliary_task_norm_not_norm': False, 'auxilliary_task_norm_not_norm-dense_dim_2': None, 'auxilliary_task_norm_not_norm-dense_dim': None, 'dense_dim_auxilliary_pos': 0, 'auxilliary_task_pos': False}, 'char_embedding_dim': 100, 'extend_vocab_with_test': True, 'symbolic_root': True, 'shared_context': 'all', 'hidden_size_encoder': 200, 'proportion_pred_train': None, 'output_dim': 200, 'word_voc_input_size': 20001, 'batch_size': 500, 'lr': 0.001, 'ponderation_normalize_loss': 1, 'encoder_arch': {'cell_word': 'LSTM', 'word_embedding_projected_dim': None, 'word_embed': False, 'n_layers_word_encoder': 1, 'word_embed_init': None, 'dropout_sent_encoder_cell': 0.5, 'word_embedding_dim': 0, 'dir_word_encoder': 2, 'n_layers_sent_cell': 2, 'dir_sent_encoder': 2, 'drop_out_sent_encoder_out': 0.0, 'cell_sentence': 'LSTM', 'drop_out_word_encoder_out': 0.5, 'dropout_word_encoder_cell': 0.3}, 'hidden_size_decoder': 100, 'symbolic_end': True, 'word_voc_output_size': None, 'tasks_schedule_policy': None, 'tasks': ['normalize'], 'weight_binary_loss': 0, 'n_trainable_parameters': 1021917, 'gradient_clipping': 1, 'decoder_arch': {'drop_out_char_embedding_decoder': 0.1, 'cell_sentence': 'none', 'cell_word': 'LSTM', 'word_decoding': False, 'teacher_force': True, 'char_src_attention': True, 'drop_out_word_decoder_cell': 0.3, 'drop_out_bridge': 0.5, 'dense_dim_word_pred': None, 'char_decoding': True, 'activation_char_decoder': 'nn.LeakyReLU', 'dense_dim_word_pred_2': None, 'unrolling_word': True, 'dir_word': 'uni', 'dense_dim_word_pred_3': None, 'stable_decoding_state': False, 'activation_word_decoder': 'nn.LeakyReLU', 'init_context_decoder': True}, 'weight_pos_loss': 0}, 'checkpoint_dir': '/scratch/bemuller/mt_norm_parse/env/../checkpoints/100147_rioc--DEBUG_NO_LOSS_PADDING-0-model_1-model_1_8b89-folder/100147_rioc--DEBUG_NO_LOSS_PADDING-0-model_1-model_1_8b89-1st_train-46of50epoch-checkpoint.pt'} \n",
      "WARNING : stable_decoding_state is False\n",
      "WARNING : init_context_decoder is True\n",
      "WARNING : DECODER unrolling_word is True\n",
      "WARNING : DECODER char_src_attention is True\n",
      "WARNING : DECODER word_recurrent_cell hidden dim will be {} (we added hidden_size_decoder) because of attention\n",
      "MODEL loading existing weights from checkpoint /Users/bemuller/Documents/Work/INRIA/dev/mt_norm_parse/env/../checkpoints/100147_rioc--DEBUG_NO_LOSS_PADDING-0-model_1-model_1_8b89-folder/100147_rioc--DEBUG_NO_LOSS_PADDING-0-model_1-model_1_8b89-1st_train-46of50epoch-checkpoint.pt \n",
      "INFO : dictionary is None so setting char_dictionary to model.char_dictionary\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bemuller/anaconda3/envs/torch_env/lib/python3.5/site-packages/torch/nn/modules/rnn.py:38: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n",
      "/Users/bemuller/anaconda3/envs/torch_env/lib/python3.5/site-packages/torch/nn/modules/rnn.py:38: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please type what you want to normalize word by word and finishes by 'stop' ? to end type : 'END'    my friend is a anwesome girld isn't she ?\n",
      "Please type what you want to normalize word by word and finishes by 'stop' ? to end type : 'END'    stop\n",
      "sent [\"my friend is a anwesome girld isn't she ?\"]\n",
      "Attention shape torch.Size([3, 18, 20])\n",
      "Attention tensor([[[ 0.4288,  0.5068,  0.0645,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  1.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  1.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         ...,\n",
      "         [ 0.0000,  0.0000,  1.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  1.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  1.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "        [[ 0.1269,  0.0707,  0.0525,  ...,  0.0292,  0.0256,  0.0123],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  1.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0004,  0.0000,  0.9996],\n",
      "         ...,\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  1.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  1.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  1.0000]],\n",
      "\n",
      "        [[ 0.4359,  0.4909,  0.0732,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  1.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  1.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         ...,\n",
      "         [ 0.0000,  0.0000,  1.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  1.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  1.0000,  ...,  0.0000,  0.0000,  0.0000]]]) [['_ROOT_CHAR'], ['m', 'y', ' ', 'f', 'r', 'i', 'e', 'n', 'd', ' ', 'i', 's', ' ', 'a', ' ', 'a', 'n', 'w'], ['_END']] ['_ROOT_CHAR', 'Mon plait de la pl', '_END']\n",
      "DECODED text is : ['_ROOT_CHAR', 'Mon plait de la pl', '_END'] original is ['_ROOT_CHAR', 'my friend is a anw', '_END'] and None seen as word embed \n",
      "Please type what you want to normalize word by word and finishes by 'stop' ? to end type : 'END'    and you what's sup \n",
      "Please type what you want to normalize word by word and finishes by 'stop' ? to end type : 'END'    stop\n",
      "sent [\"and you what's sup \"]\n",
      "Attention shape torch.Size([3, 18, 20])\n",
      "Attention tensor([[[ 0.4288,  0.5068,  0.0645,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  1.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  1.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         ...,\n",
      "         [ 0.0000,  0.0000,  1.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  1.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  1.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "        [[ 0.0901,  0.1012,  0.0862,  ...,  0.0190,  0.0133,  0.0107],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0001,  0.9999],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  1.0000],\n",
      "         ...,\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  1.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  1.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  1.0000]],\n",
      "\n",
      "        [[ 0.4359,  0.4909,  0.0732,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  1.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  1.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         ...,\n",
      "         [ 0.0000,  0.0000,  1.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  1.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  1.0000,  ...,  0.0000,  0.0000,  0.0000]]]) [['_ROOT_CHAR'], ['a', 'n', 'd', ' ', 'y', 'o', 'u', ' ', 'w', 'h', 'a', 't', \"'\", 's', ' ', 's', 'u', 'p'], ['_END']] ['_ROOT_CHAR', 'pourrait être au m', '_END']\n",
      "DECODED text is : ['_ROOT_CHAR', 'pourrait être au m', '_END'] original is ['_ROOT_CHAR', \"and you what's sup\", '_END'] and None seen as word embed \n",
      "Please type what you want to normalize word by word and finishes by 'stop' ? to end type : 'END'    what is your idea \n",
      "Please type what you want to normalize word by word and finishes by 'stop' ? to end type : 'END'    stop\n",
      "sent ['what is your idea ']\n",
      "Attention shape torch.Size([3, 18, 20])\n",
      "Attention tensor([[[ 0.4288,  0.5068,  0.0645,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  1.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  1.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         ...,\n",
      "         [ 0.0000,  0.0000,  1.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  1.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  1.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "        [[ 0.0570,  0.0528,  0.0516,  ...,  0.0460,  0.0857,  0.0124],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  1.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  1.0000],\n",
      "         ...,\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  1.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  1.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  1.0000]],\n",
      "\n",
      "        [[ 0.4359,  0.4909,  0.0732,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  1.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  1.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         ...,\n",
      "         [ 0.0000,  0.0000,  1.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  1.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  1.0000,  ...,  0.0000,  0.0000,  0.0000]]]) [['_ROOT_CHAR'], ['w', 'h', 'a', 't', ' ', 'i', 's', ' ', 'y', 'o', 'u', 'r', ' ', 'i', 'd', 'e', 'a', ' '], ['_END']] ['_ROOT_CHAR', \"C'est un peu de la\", '_END']\n",
      "DECODED text is : ['_ROOT_CHAR', \"C'est un peu de la\", '_END'] original is ['_ROOT_CHAR', 'what is your idea ', '_END'] and None seen as word embed \n",
      "Please type what you want to normalize word by word and finishes by 'stop' ? to end type : 'END'    my name is awesome\n",
      "Please type what you want to normalize word by word and finishes by 'stop' ? to end type : 'END'    stop\n",
      "sent ['my name is awesome']\n",
      "Attention shape torch.Size([3, 18, 20])\n",
      "Attention tensor([[[ 0.4288,  0.5068,  0.0645,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  1.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  1.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         ...,\n",
      "         [ 0.0000,  0.0000,  1.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  1.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  1.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "        [[ 0.1028,  0.0477,  0.0327,  ...,  0.0294,  0.0294,  0.0103],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  1.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  1.0000],\n",
      "         ...,\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  1.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  1.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  1.0000]],\n",
      "\n",
      "        [[ 0.4359,  0.4909,  0.0732,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  1.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  1.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         ...,\n",
      "         [ 0.0000,  0.0000,  1.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  1.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  1.0000,  ...,  0.0000,  0.0000,  0.0000]]]) [['_ROOT_CHAR'], ['m', 'y', ' ', 'n', 'a', 'm', 'e', ' ', 'i', 's', ' ', 'a', 'w', 'e', 's', 'o', 'm', 'e'], ['_END']] ['_ROOT_CHAR', 'Ma mais je ne suis', '_END']\n",
      "DECODED text is : ['_ROOT_CHAR', 'Ma mais je ne suis', '_END'] original is ['_ROOT_CHAR', 'my name is awesome', '_END'] and None seen as word embed \n",
      "Please type what you want to normalize word by word and finishes by 'stop' ? to end type : 'END'    i'm a real smoker\n",
      "Please type what you want to normalize word by word and finishes by 'stop' ? to end type : 'END'    stop\n",
      "sent [\"i'm a real smoker\"]\n",
      "Attention shape torch.Size([3, 18, 19])\n",
      "Attention tensor([[[ 0.4288,  0.5068,  0.0645,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  1.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  1.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         ...,\n",
      "         [ 0.0000,  0.0000,  1.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  1.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  1.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "        [[ 0.0729,  0.0737,  0.0778,  ...,  0.0561,  0.0321,  0.0093],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  1.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  1.0000],\n",
      "         ...,\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  1.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  1.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  1.0000]],\n",
      "\n",
      "        [[ 0.4359,  0.4909,  0.0732,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  1.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  1.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         ...,\n",
      "         [ 0.0000,  0.0000,  1.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  1.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  1.0000,  ...,  0.0000,  0.0000,  0.0000]]]) [['_ROOT_CHAR'], ['i', \"'\", 'm', ' ', 'a', ' ', 'r', 'e', 'a', 'l', ' ', 's', 'm', 'o', 'k', 'e', 'r'], ['_END']] ['_ROOT_CHAR', 'Je suis de la plus', '_END']\n",
      "DECODED text is : ['_ROOT_CHAR', 'Je suis de la plus', '_END'] original is ['_ROOT_CHAR', \"i'm a real smoker\", '_END'] and None seen as word embed \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please type what you want to normalize word by word and finishes by 'stop' ? to end type : 'END'    i'm\n",
      "Please type what you want to normalize word by word and finishes by 'stop' ? to end type : 'END'    sto\n",
      "Please type what you want to normalize word by word and finishes by 'stop' ? to end type : 'END'    stop\n",
      "sent [\"i'm\", 'sto']\n",
      "Attention shape torch.Size([4, 18, 5])\n",
      "Attention tensor([[[ 0.4288,  0.5068,  0.0645,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  1.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  1.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  1.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  1.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  1.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  1.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  1.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  1.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  1.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  1.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  1.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  1.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  1.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  1.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  1.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  1.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  1.0000,  0.0000,  0.0000]],\n",
      "\n",
      "        [[ 0.3074,  0.2197,  0.3075,  0.1258,  0.0395],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  1.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  1.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  1.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  1.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  1.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  1.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  1.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  1.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  1.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  1.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  1.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  1.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  1.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  1.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  1.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  1.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  1.0000]],\n",
      "\n",
      "        [[ 0.2474,  0.2714,  0.2492,  0.1974,  0.0346],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  1.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  1.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  1.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  1.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  1.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  1.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  1.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  1.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  1.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  1.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  1.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  1.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  1.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  1.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  1.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  1.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  1.0000]],\n",
      "\n",
      "        [[ 0.4359,  0.4909,  0.0732,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  1.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  1.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  1.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  1.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  1.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  1.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  1.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  1.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  1.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  1.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  1.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  1.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  1.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  1.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  1.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  1.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  1.0000,  0.0000,  0.0000]]]) [['_ROOT_CHAR'], ['i', \"'\", 'm'], ['s', 't', 'o'], ['_END']] ['_ROOT_CHAR', 'ta la plus de la p', 'sir le monde de la', '_END']\n",
      "DECODED text is : ['_ROOT_CHAR', 'ta la plus de la p', 'sir le monde de la', '_END'] original is ['_ROOT_CHAR', \"i'm\", 'sto', '_END'] and None seen as word embed \n",
      "Please type what you want to normalize word by word and finishes by 'stop' ? to end type : 'END'    i'm red\n",
      "Please type what you want to normalize word by word and finishes by 'stop' ? to end type : 'END'    stop\n",
      "sent [\"i'm red\"]\n",
      "Attention shape torch.Size([3, 18, 9])\n",
      "Attention tensor([[[ 0.4288,  0.5068,  0.0645,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "           0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  1.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "           0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  1.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "           0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  1.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "           0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  1.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "           0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  1.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "           0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  1.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "           0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  1.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "           0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  1.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "           0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  1.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "           0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  1.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "           0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  1.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "           0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  1.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "           0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  1.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "           0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  1.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "           0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  1.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "           0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  1.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "           0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  1.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "           0.0000,  0.0000]],\n",
      "\n",
      "        [[ 0.1255,  0.1272,  0.1898,  0.1598,  0.1985,  0.0645,  0.0798,\n",
      "           0.0400,  0.0150],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "           0.0000,  1.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "           0.0000,  1.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "           0.0000,  1.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "           0.0000,  1.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "           0.0000,  1.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "           0.0000,  1.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "           0.0000,  1.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "           0.0000,  1.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "           0.0000,  1.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "           0.0000,  1.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "           0.0000,  1.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "           0.0000,  1.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "           0.0000,  1.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "           0.0000,  1.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "           0.0000,  1.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "           0.0000,  1.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "           0.0000,  1.0000]],\n",
      "\n",
      "        [[ 0.4359,  0.4909,  0.0732,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "           0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  1.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "           0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  1.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "           0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  1.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "           0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  1.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "           0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  1.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "           0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  1.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "           0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  1.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "           0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  1.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "           0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  1.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "           0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  1.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "           0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  1.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "           0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  1.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "           0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  1.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "           0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  1.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "           0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  1.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "           0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  1.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "           0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  1.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "           0.0000,  0.0000]]]) [['_ROOT_CHAR'], ['i', \"'\", 'm', ' ', 'r', 'e', 'd'], ['_END']] ['_ROOT_CHAR', 'Je suis de la plus', '_END']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DECODED text is : ['_ROOT_CHAR', 'Je suis de la plus', '_END'] original is ['_ROOT_CHAR', \"i'm red\", '_END'] and None seen as word embed \n",
      "Please type what you want to normalize word by word and finishes by 'stop' ? to end type : 'END'    end\n"
     ]
    }
   ],
   "source": [
    "for folder_name in list_:\n",
    "    assert len(list_)>0, \"list empty\"\n",
    "    model_full_name = folder_name[:-7]\n",
    "    print(\"\\n\\n--------------------------------Interatcing with new model--------------------------------\\n\", model_full_name,\"\\n\\n\")\n",
    "    dic_path = os.path.join(script_dir, \"..\", \"checkpoints\", model_full_name + \"-folder\", \"dictionaries\")\n",
    "    model_dir = os.path.join(script_dir, \"..\", \"checkpoints\", model_full_name + \"-folder\")\n",
    "    interact(dic_path=dic_path, dir_model=model_dir, model_full_name=model_full_name, \n",
    "             debug=False, \n",
    "             show_attention=False, save_attention=False,\n",
    "             extra_arg_specific_label=\"\",\n",
    "             verbose=0)\n",
    "    #break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comments\n",
    "- on model 97077_rioc-ee386-REP_-replicate1-1dir-scale_1-model_15_f19e with auxilliary task no attention quite small model\n",
    "    - interesting case : \n",
    "        - my normalized to tomorrow : probably bcause of 2mor --> tommorow \n",
    "    - very lame at decoding very short word ? \n",
    "- 97147_rioc-6ed16-ATTCONTEXT-with_att-dir_2-X-dropout_bridge0.1-context_word-model_4_76a1\n",
    "    - attention is not static --> but no clear trend\n",
    "    - interetsintgly with same model but only using sent context : the attention focus on last word all the time \"97147_rioc-6ed16-ATTCONTEXT-with_att-dir_2-X-dropout_bridge0.1-context_sent-model_2_7d92\" \n",
    "    \n",
    "- \"97184_rioc-0cc46-ATTstandart_passing-with_att-dir_2-X-context_all-stable_decoding_state-init_context_decoder_False-model_4_2ffa\"\n",
    "    - the best model you played with so far\n",
    "    - classification does not agree with seq predictiion : most of the classification is NORMED while it performs some change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage: open [-e] [-t] [-f] [-W] [-R] [-n] [-g] [-h] [-s <partial SDK name>][-b <bundle identifier>] [-a <application>] [filenames] [--args arguments]\r\n",
      "Help: Open opens files from a shell.\r\n",
      "      By default, opens each file using the default application for that file.  \r\n",
      "      If the file is in the form of a URL, the file will be opened as a URL.\r\n",
      "Options: \r\n",
      "      -a                Opens with the specified application.\r\n",
      "      -b                Opens with the specified application bundle identifier.\r\n",
      "      -e                Opens with TextEdit.\r\n",
      "      -t                Opens with default text editor.\r\n",
      "      -f                Reads input from standard input and opens with TextEdit.\r\n",
      "      -F  --fresh       Launches the app fresh, that is, without restoring windows. Saved persistent state is lost, excluding Untitled documents.\r\n",
      "      -R, --reveal      Selects in the Finder instead of opening.\r\n",
      "      -W, --wait-apps   Blocks until the used applications are closed (even if they were already running).\r\n",
      "          --args        All remaining arguments are passed in argv to the application's main() function instead of opened.\r\n",
      "      -n, --new         Open a new instance of the application even if one is already running.\r\n",
      "      -j, --hide        Launches the app hidden.\r\n",
      "      -g, --background  Does not bring the application to the foreground.\r\n",
      "      -h, --header      Searches header file locations for headers matching the given filenames, and opens them.\r\n",
      "      -s                For -h, the SDK to use; if supplied, only SDKs whose names contain the argument value are searched.\r\n",
      "                        Otherwise the highest versioned SDK in each platform is used.\r\n"
     ]
    }
   ],
   "source": [
    "!open"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"./dat"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
