{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline  \n",
    "import sys\n",
    "sys.path.insert(0,\"..\")\n",
    "from predict.prediction_batch import greedy_decode_batch\n",
    "from predict.prediction_string import decode_seq_str, decode_interacively\n",
    "import pdb\n",
    "from model.loss import LossCompute\n",
    "import os\n",
    "from io_.info_print import printing\n",
    "from model.seq2seq import LexNormalizer\n",
    "from model.generator import Generator\n",
    "from evaluate.interact import interact\n",
    "MAX_LEN = 20\n",
    "script_dir = \".\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_folder_starts_with = \"f2f2-batchXdropout_char0.1-to_char_src-1_dir_sent-10_batch_size-model_18_aa04\"\n",
    "#model_folder_starts_with = \"f178-DROPOUT_EVEN_INCREASE-0.1-to_sent+word+bridge_out-model_3_046c\"\n",
    "#model_folder_starts_with = \"8ce6b-extend_ep-get_True-attention_simplifiedXauxXdropout0.1_scale_aux-True_aux-0.1do_char_dec-True_char_src_atten-model_14_ad6c\"\n",
    "#model_folder_starts_with = \"a5c77\"\n",
    "#model_folder_starts_with = \"fef8-new_data-batchXdropout_char0-to_char_src-1_dir_sent-10_batch_size-model_2_51a5\"\n",
    "#model_folder_starts_with = \"e3900-liu-attention+unrolling0.2-to_char_src-1_dir_sent-15_batch-dir_word_src_1-unrolling_word_True-char_src_attention_False-model_2_14c2\"\n",
    "#model_folder_starts_with = \"e3900-liu-attention+unrolling0.2-to_char_src-1_dir_sent-15_batch-dir_word_src_1-unrolling_word_True-char_src_attention_True-model_3_1a3\"\n",
    "#model_folder_starts_with =\"8ce6b\"\n",
    "#model_folder_starts_with = \"8e628\"\n",
    "#model_folder_starts_with = \"84736\"\n",
    "#model_folder_starts_with = \"84736-84736-SENT_context-get_True-attention_simplifiedXdrop_out_charXdir_word1dir_word-False_aux-0do_char_dec-True_char_src_atten-model_10_b59b\"\n",
    "#model_folder_starts_with = \"84736-84736-SENT_context-get_True-attention_simplifiedXdrop_out_charXdir_word1dir_word-False_aux-0do_char_dec-False_char_src_atten-model_9_bb70\"\n",
    "model_folder_starts_with = \"b9e49-aux_report+dense+ponderatipn+no_bucketing100-dense_dim_auxilliary0.001weight_binary_loss0.2-to_char_src-1_dir_sent-10_batch-dir_word_src_1-unrolling_word_True-char_src_attention_False-model_9_0f05\"\n",
    "#model_folder_starts_with = \"97068_rioc-b91d7-aux-again-biggerREPLICATE-replicate1-2dir_word-None_aux-model_19_fb56\"\n",
    "#model_folder_starts_with = \"97079_rioc-ef365-ATTbest-scale-2-True-25dir_word_encoder-all_context-att2-model_1_cade\"\n",
    "#model_folder_starts_with = \"97077_rioc-ee386-REP_-replicate1-1dir-scale_1-model_15_f19e\"\n",
    "#model_folder_starts_with = \"97147_rioc-6ed16-ATTCONTEXT-with_att-dir_2-X-dropout_bridge0.1-context_word-model_4_76a1\"\n",
    "#model_folder_starts_with = \"97147_rioc-6ed16-ATTCONTEXT-with_att-dir_2-X-dropout_bridge0.1-context_sent-model_2_7d92\"\n",
    "#model_folder_starts_with = \"97147_rioc-6ed16-ATTCONTEXT-with_att-dir_2-X-dropout_bridge0.1-context_word-model_4_76a1\"\n",
    "#model_folder_starts_with = \"97184_rioc-0cc46-ATTstandart_passing-with_att-dir_2-X-context_all-stable_decoding_state-init_context_decoder_False-model_4_2ffa\"\n",
    "#model_folder_starts_with = \"97221_rioc-3e4f2-ATTattentionXauxXcont-big-teacher-with_att-dir_2-X-context_word-stable_decod-init_con_False-teacher_forceprop10_True-model_4_c551\"\n",
    "#model_folder_starts_with = \"97221_rioc-3e4f2-ATTattentionXauxXcont-big-teacher-with_att-dir_2-X-context_all-stable_decod-init_con_False-teacher_forceprop10_True-model_2_7673\"\n",
    "#model_folder_starts_with = \"97555_rioc--ATTword_vs_char-scale2-sha_context_all-auxnorm_not_norm_True-word_de_False-model_6_c253-folder\"\n",
    "#model_folder_starts_with = \"97555_rioc--ATTword_vs_char-scale2-sha_context_all-auxnorm_not_norm_True-word_de_True-model_5_72ee-folder\"\n",
    "model_folder_starts_with = \"97184_rioc-0cc46-ATTstandart_passing-with_att-dir_2-X-context_all-stable_decoding_state-init_context_decoder_False-model_4_2ffa\"\n",
    "model_folder_starts_with = \"97679_rioc--ATTmodel_10-model_10_b22f-folder\"\n",
    "model_folder_starts_with = \"4e128-WARMUP-unrolling-False0-model_1-model_1_1660-folder\"\n",
    "model_folder_starts_with = \"97734_rioc--DEBUG-2LSMT-2dense0-model_3-model_3_88a4-folder\"\n",
    "#model_folder_starts_with = \"97734_rioc--DEBUG-2LSMT-2dense0-model_1-model_1_413f-folder\"\n",
    "#model_folder_starts_with = \"97734_rioc--DEBUG-2LSMT-2dense0-model_2-model_2_7a98\"\n",
    "model_folder_starts_with = \"97793_rioc--DEBUG_NO_LOSS_PADDING-2LSMT-2dense0-model_1-model_1_b872-folder\"\n",
    "model_folder_starts_with = \"97823_rioc--DEBUG_NO_LOSS_PADDING-LEAKY-2LSMT-2dense-5DROPOUT00-model_1-model_1_60d8-folder\"\n",
    "\n",
    "\n",
    "# model word context char decode no attn \n",
    "#model_folder_starts_with = \"97942_rioc--DEBUG_NO_LOSS_PADDING-0-model_1-model_1_767d-folder\"\n",
    "model_folder_starts_with = \"98349_rioc--DEBUG_NO_LOSS_PADDING-0-model_1-model_1_6249-folder\"\n",
    "model_folder_starts_with = \"98759_rioc--DEBUG_NO_LOSS_PADDING-0-model_1-model_1_b108-folder\"\n",
    "model_folder_starts_with = \"99428_rioc--DEBUG_NO_LOSS_PADDING-0-model_1-model_1_8fb8-folder\"\n",
    "#model_folder_starts_with = \"99428_rioc--DEBUG_NO_LOSS_PADDING-0-model_1-model_1_8fb8-start_3_ep-X1-train_longer\"\n",
    "model_folder_starts_with = \"99428_rioc--DEBUG_NO_LOSS_PADDING-0-model_1-model_1_8fb8-folder\"\n",
    "#\"98759_rioc--DEBUG_NO_LOSS_PADDING-0-model_1-model_1_b108-folder\"\n",
    "model_folder_starts_with = \"99733_rioc--DEBUG_NO_LOSS_PADDING-0-model_3-model_3_ccda-folder\"\n",
    "model_folder_starts_with =  \"99731_rioc--DEBUG_NO_LOSS_PADDING-0-model_1-model_1_6cf0-folder\"\n",
    "#model_folder_starts_with = \"99733_rioc--DEBUG_NO_LOSS_PADDING-0-model_4-model_4_5ce2-folder\"\n",
    "# fake words only char2char\n",
    "#model_folder_starts_with = \"EU\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['99731_rioc--DEBUG_NO_LOSS_PADDING-0-model_1-model_1_6cf0-folder']\n"
     ]
    }
   ],
   "source": [
    "# list all models folder that starts with model_folder_starts_with\n",
    "list_all_dir = os.listdir(\"../checkpoints/\")\n",
    "list_ = [dir_ for dir_ in list_all_dir if dir_.startswith(model_folder_starts_with) and not dir_.endswith(\"log\") and not dir_.endswith(\"summary\")]\n",
    "print(list_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model to interact with \n",
    "\n",
    "#### Former models : trained on Liu only , source added as concatanation of word and sentence level as h_0 of decoder , batch size = 10 , small drop out , \n",
    "\n",
    "- f2f2-batchXdropout_char0.1-to_char_src-1_dir_sent-10_batch_size-model_18_aa04 : very bad at interacting (NB ; pb of eval)\n",
    "- f178 also good model trained on liu only \n",
    "- 8e628 : attention ; no aux (no bucket , get_batch False ) :  - attention degrades abit the results (still feedin char embedding also)\n",
    "- e390 + same 24f94 goo : \n",
    "    - best is e3900-liu-attention+unrolling0.2-to_char_src-1_dir_sent-15_batch-dir_word_src_1-unrolling_word_True-char_src_attention_False-model_2_14c2 \n",
    "    - same with attentin but lame : e3900-liu-attention+unrolling0.2-to_char_src-1_dir_sent-15_batch-dir_word_src_1-unrolling_word_True-char_src_attention_True-model_3_1a3f (have to do some code for reloading and visulizing !)\n",
    "\n",
    "#### New data \n",
    "- fef8_new_data : \n",
    "- mixed data+ aix test :  8d9a0 + b9e49 to compare bucketing impact at train time \n",
    "    - /Users/bemuller/Documents/Work/INRIA/dev/mt_norm_parse/reports/aux_report+dense+ponderatipn+bucketing-last-report-data.html or file:///Users/bemuller/Documents/Work/INRIA/dev/mt_norm_parse/reports/aux_report+dense+ponderatipn+bucketing-last-report-norm.html\n",
    "    - /Users/bemuller/Documents/Work/INRIA/dev/mt_norm_parse/reports/aux_report+dense+ponderatipn+no_bucketing-last-report-data.html or file:///Users/bemuller/Documents/Work/INRIA/dev/mt_norm_parse/reports/8d9a0-auxiililary_true_false.html\n",
    "    \n",
    "    \n",
    "- Auxilliary tuning : best model : b9e49-aux_report+dense+ponderatipn+no_bucketing100-dense_dim_auxilliary0.001weight_binary_loss0.2-to_char_src-1_dir_sent-10_batch-dir_word_src_1-unrolling_word_True-char_src_attention_False-model_9_0f05-folderfile:///Users/bemuller/Documents/Work/INRIA/dev/mt_norm_parse/reports/aux_report+dense+ponderatipn+no_bucketing-last-report-enriched-auxilliary_task_norm_not_norm-dense_dim_view.html\n",
    "-\n",
    "\n",
    "#### Last two ablations with extending epoch + mix data + ablation on auxilliary task ponderation + attention or not  \n",
    "\n",
    "- 8ce6b-extend_ep... \n",
    "    - super lame attention model (from ablation with all context)\n",
    "    - the other one a bit better   \n",
    "- a5c77 a bit better but still : no attention much better : cf. plot \n",
    "\n",
    "#### Smaller model + liu only \n",
    "- 84736 smaller model: still waiting for models : attention not helping \n",
    "\n",
    "NB : attention makes training 10 times slower  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!open /Users/bemuller/Documents/Work/INRIA/dev/mt_norm_parse/reports/ablation_DROPOUT_analysis.html # f178\n",
    "#!open /Users/bemuller/Documents/Work/INRIA/dev/mt_norm_parse/reports/ablation_DROPOUT_analysis_norm_view.html #f178\n",
    "#!open /Users/bemuller/Documents/Work/INRIA/dev/mt_norm_parse/reports/ablation_DROPOUT_analysis_2.html # f178 + aaad \n",
    "\n",
    "#!open /Users/bemuller/Documents/Work/INRIA/dev/mt_norm_parse/reports/f2f2-batchXdropout_char-summary-norm_view.html\n",
    "#!open /Users/bemuller/Documents/Work/INRIA/dev/mt_norm_parse/reports/f2f2-batchXdropout_char-summary-data_view.html\n",
    "# -- \n",
    "#!open /Users/bemuller/Documents/Work/INRIA/dev/mt_norm_parse/reports/f2f2_best+01880_reproduction-VAL_TRUE.html\n",
    "##file:///Users/bemuller/Documents/Work/INRIA/dev/mt_norm_parse/reports/f2f2-iterate+new_data-norm_view.html (different results with below ? get_batch due ?? )\n",
    "##/Users/bemuller/Documents/Work/INRIA/dev/mt_norm_parse/reports/f2f2-batchXdropout_char-summary-norm_view.html \n",
    "## diffent from file:///Users/bemuller/Documents/Work/INRIA/dev/mt_norm_parse/reports/f2f2_best+01880_reproduction-VAL_TRUE.html val True \n",
    "#--\n",
    "\n",
    "#!open /Users/bemuller/Documents/Work/INRIA/dev/mt_norm_parse/reports/liu-attention+unrolling-more-param.html #e390\n",
    "#!open file:///Users/bemuller/Documents/Work/INRIA/dev/mt_norm_parse/reports/e390_best+24f9d-VAL_FALSE.html (e390 with Vale False 10 points above!! )\n",
    "\n",
    "#!open /Users/bemuller/Documents/Work/INRIA/dev/mt_norm_parse/reports/fef8_new_data.html (very good ?)\n",
    "\n",
    "#!open /Users/bemuller/Documents/Work/INRIA/dev/mt_norm_parse/reports/extend_ep-SENT_context-get_True-attention_simplifiedXauxXdropout-last+bucket_False_eval-get_batch_False.html\n",
    "\n",
    "#!open /Users/bemuller/Documents/Work/INRIA/dev/mt_norm_parse/reports/8ce6b-extend_ep-get_True-attention_simplifiedXauxXdropout.html\n",
    "\n",
    "#!open /Users/bemuller/Documents/Work/INRIA/dev/mt_norm_parse/reports/8e628-no_bucketing-get_batch_False-train-attention-last+bucket_False_eval-get_batch_False-report.json.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "--------------------------------Interatcing with new model--------------------------------\n",
      " 99731_rioc--DEBUG_NO_LOSS_PADDING-0-model_1-model_1_6cf0 \n",
      "\n",
      "\n",
      "Loading dictionary from ./../checkpoints/99731_rioc--DEBUG_NO_LOSS_PADDING-0-model_1-model_1_6cf0-folder/dictionaries \n",
      "WARNING : checkpoint_dir as indicated in args.json is not found, lt checkpoint_dir /\n",
      "Loading model with argument {'symbolic_root': True, 'voc_size': 116, 'char_embedding_dim': 40, 'proportion_pred_train': None, 'hidden_size_encoder': 20, 'auxilliary_arch': {'weight_binary_loss': 1, 'dense_dim_auxilliary_pos': None, 'dense_dim_auxilliary_pos_2': None, 'auxilliary_task_norm_not_norm': True, 'auxilliary_task_norm_not_norm-dense_dim': None, 'auxilliary_task_pos': False, 'auxilliary_task_norm_not_norm-dense_dim_2': None}, 'n_trainable_parameters': 36310, 'lr': 0.001, 'encoder_arch': {'dir_word_encoder': 2, 'word_embed_init': None, 'drop_out_sent_encoder_out': 0.0, 'dropout_word_encoder_cell': 0.0, 'word_embed': False, 'dir_sent_encoder': 2, 'word_embedding_projected_dim': None, 'word_embedding_dim': 0, 'cell_sentence': 'LSTM', 'n_layers_sent_cell': 1, 'dropout_sent_encoder_cell': 0, 'drop_out_word_encoder_out': 0.0, 'cell_word': 'LSTM', 'n_layers_word_encoder': 1}, 'hidden_size_sent_encoder': 24, 'batch_size': 10, 'decoder_arch': {'dense_dim_word_pred': None, 'char_decoding': True, 'cell_sentence': 'none', 'drop_out_char_embedding_decoder': 0.0, 'dense_dim_word_pred_2': None, 'activation_word_decoder': 'nn.LeakyReLU', 'drop_out_word_decoder_cell': 0.0, 'dir_word': 'uni', 'init_context_decoder': True, 'stable_decoding_state': False, 'char_src_attention': True, 'drop_out_bridge': 0.0, 'activation_char_decoder': 'nn.LeakyReLU', 'dense_dim_word_pred_3': None, 'unrolling_word': True, 'teacher_force': [True, False], 'cell_word': 'LSTM', 'word_decoding': False}, 'word_voc_input_size': 1892, 'tasks_schedule_policy': None, 'lr_policy': 'lr_constant', 'shared_context': 'word', 'hidden_size_decoder': 30, 'weight_binary_loss': 0.01, 'ponderation_normalize_loss': 1, 'word_voc_output_size': None, 'gradient_clipping': 1, 'extend_vocab_with_test': True, 'symbolic_end': True, 'output_dim': 20, 'weight_pos_loss': 0.0}\n",
      "Model arguments are {'info_checkpoint': {'proportion_pred_train': None, 'other': {'weight_binary_loss': 0.01, 'optim_strategy': 'lr_constant', 'average_per_epoch(min)': '0.58', 'extend_n_batch': 2, 'data': 'dev', 'error_curves_details': '/scratch/bemuller/mt_norm_parse/env/../checkpoints/99731_rioc--DEBUG_NO_LOSS_PADDING-0-model_1-model_1_6cf0-folder/99731_rioc--DEBUG_NO_LOSS_PADDING-0-model_1-model_1_6cf0-details-last-plo-seq.png', 'seed(np/torch)': [123, 123], 'ponderation_normalize_loss': 1, 'lr': 0.001, 'error_curves': '/scratch/bemuller/mt_norm_parse/env/../checkpoints/99731_rioc--DEBUG_NO_LOSS_PADDING-0-model_1-model_1_6cf0-folder/99731_rioc--DEBUG_NO_LOSS_PADDING-0-model_1-model_1_6cf0-last-plo-seq.png', 'time_training(min)': '29.37', 'loss': 0.003959220872262673, 'weight_pos_loss': 0.0}, 'tasks_schedule_policy': None, 'gradient_clipping': 1, 'dev_data_path': '/scratch/bemuller/mt_norm_parse/env/.././data/LiLiu/2577_tweets-li-sent-dev_500.conll', 'git_id': '88b0413f28ddb1a912bbd808a94467adf82cd74f', 'train_data_path': '/scratch/bemuller/mt_norm_parse/env/.././data/LiLiu/2577_tweets-li-sent-train_2009.conll', 'teacher_force': [True, False], 'n_epochs': 50, 'batch_size': 10}, 'hyperparameters': {'symbolic_root': True, 'voc_size': 116, 'char_embedding_dim': 40, 'proportion_pred_train': None, 'hidden_size_encoder': 20, 'auxilliary_arch': {'weight_binary_loss': 1, 'dense_dim_auxilliary_pos': None, 'dense_dim_auxilliary_pos_2': None, 'auxilliary_task_norm_not_norm': True, 'auxilliary_task_norm_not_norm-dense_dim': None, 'auxilliary_task_pos': False, 'auxilliary_task_norm_not_norm-dense_dim_2': None}, 'n_trainable_parameters': 36310, 'lr': 0.001, 'encoder_arch': {'dir_word_encoder': 2, 'word_embed_init': None, 'drop_out_sent_encoder_out': 0.0, 'dropout_word_encoder_cell': 0.0, 'word_embed': False, 'dir_sent_encoder': 2, 'word_embedding_projected_dim': None, 'word_embedding_dim': 0, 'cell_sentence': 'LSTM', 'n_layers_sent_cell': 1, 'dropout_sent_encoder_cell': 0, 'drop_out_word_encoder_out': 0.0, 'cell_word': 'LSTM', 'n_layers_word_encoder': 1}, 'hidden_size_sent_encoder': 24, 'batch_size': 10, 'decoder_arch': {'dense_dim_word_pred': None, 'char_decoding': True, 'cell_sentence': 'none', 'drop_out_char_embedding_decoder': 0.0, 'dense_dim_word_pred_2': None, 'activation_word_decoder': 'nn.LeakyReLU', 'drop_out_word_decoder_cell': 0.0, 'dir_word': 'uni', 'init_context_decoder': True, 'stable_decoding_state': False, 'char_src_attention': True, 'drop_out_bridge': 0.0, 'activation_char_decoder': 'nn.LeakyReLU', 'dense_dim_word_pred_3': None, 'unrolling_word': True, 'teacher_force': [True, False], 'cell_word': 'LSTM', 'word_decoding': False}, 'word_voc_input_size': 1892, 'tasks_schedule_policy': None, 'lr_policy': 'lr_constant', 'shared_context': 'word', 'hidden_size_decoder': 30, 'weight_binary_loss': 0.01, 'ponderation_normalize_loss': 1, 'word_voc_output_size': None, 'gradient_clipping': 1, 'extend_vocab_with_test': True, 'symbolic_end': True, 'output_dim': 20, 'weight_pos_loss': 0.0}, 'checkpoint_dir': '/scratch/bemuller/mt_norm_parse/env/../checkpoints/99731_rioc--DEBUG_NO_LOSS_PADDING-0-model_1-model_1_6cf0-folder/99731_rioc--DEBUG_NO_LOSS_PADDING-0-model_1-model_1_6cf0-1st_train-50of50epoch-checkpoint.pt'} \n",
      "WARNING : BinaryPredictor as dense_dim is None no dense layer added to norm_not_norm predictor\n",
      "WARNING : stable_decoding_state is False\n",
      "WARNING : init_context_decoder is True\n",
      "WARNING : DECODER unrolling_word is True\n",
      "WARNING : DECODER char_src_attention is True\n",
      "WARNING : DECODER word_recurrent_cell hidden dim will be {} (we added hidden_size_decoder) because of attention\n",
      "MODEL loading existing weights from checkpoint /Users/bemuller/Documents/Work/INRIA/dev/mt_norm_parse/env/../checkpoints/99731_rioc--DEBUG_NO_LOSS_PADDING-0-model_1-model_1_6cf0-folder/99731_rioc--DEBUG_NO_LOSS_PADDING-0-model_1-model_1_6cf0-1st_train-50of50epoch-checkpoint.pt \n",
      "INFO : dictionary is None so setting char_dictionary to model.char_dictionary\n",
      "Please type what you want to normalize word by word and finishes by 'stop' ? to end type : 'END'    whats\n",
      "Please type what you want to normalize word by word and finishes by 'stop' ? to end type : 'END'    up\n",
      "Please type what you want to normalize word by word and finishes by 'stop' ? to end type : 'END'    my\n",
      "Please type what you want to normalize word by word and finishes by 'stop' ? to end type : 'END'    friend\n",
      "Please type what you want to normalize word by word and finishes by 'stop' ? to end type : 'END'    are\n",
      "Please type what you want to normalize word by word and finishes by 'stop' ? to end type : 'END'    you\n",
      "Please type what you want to normalize word by word and finishes by 'stop' ? to end type : 'END'    able\n",
      "Please type what you want to normalize word by word and finishes by 'stop' ? to end type : 'END'    to\n",
      "Please type what you want to normalize word by word and finishes by 'stop' ? to end type : 'END'    learn\n",
      "Please type what you want to normalize word by word and finishes by 'stop' ? to end type : 'END'    something\n",
      "Please type what you want to normalize word by word and finishes by 'stop' ? to end type : 'END'    stop\n",
      "sent ['whats', 'up', 'my', 'friend', 'are', 'you', 'able', 'to', 'learn', 'something']\n",
      "Attention shape torch.Size([12, 18, 11])\n",
      "Attention tensor([[[ 0.4762,  0.2877,  0.2361,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.9938,  0.0025,  0.0037,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0056,  0.9930,  0.0014,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         ...,\n",
      "         [ 0.8224,  0.0385,  0.1390,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.1306,  0.8694,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.8096,  0.0128,  0.1775,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "        [[ 0.0916,  0.0558,  0.0729,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0823,  0.0051,  0.0043,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0354,  0.0488,  0.1663,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         ...,\n",
      "         [ 0.2304,  0.0158,  0.0063,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.1313,  0.0199,  0.0120,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0003,  0.0004,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "        [[ 0.3500,  0.1320,  0.1700,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.1998,  0.0009,  0.0008,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0516,  0.0052,  0.0095,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         ...,\n",
      "         [ 0.5197,  0.1034,  0.0459,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0032,  0.0023,  0.0021,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0005,  0.0008,  0.0019,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.0871,  0.0750,  0.0704,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0492,  0.0070,  0.6922,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0060,  0.0026,  0.0021,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         ...,\n",
      "         [ 0.0270,  0.0298,  0.1429,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0001,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "        [[ 0.0701,  0.0382,  0.0494,  ...,  0.1307,  0.1349,  0.2809],\n",
      "         [ 0.0009,  0.0004,  0.0034,  ...,  0.0207,  0.0153,  0.9291],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.9998],\n",
      "         ...,\n",
      "         [ 0.0000,  0.0003,  0.0000,  ...,  0.0008,  0.0002,  0.9205],\n",
      "         [ 0.0457,  0.0042,  0.1191,  ...,  0.1003,  0.1635,  0.1431],\n",
      "         [ 0.0232,  0.0086,  0.2114,  ...,  0.0494,  0.1389,  0.3346]],\n",
      "\n",
      "        [[ 0.4750,  0.2892,  0.2358,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0022,  0.0000,  0.9978,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0001,  0.9999,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         ...,\n",
      "         [ 0.0000,  0.0287,  0.9713,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0002,  0.8726,  0.1272,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.8611,  0.0085,  0.1304,  ...,  0.0000,  0.0000,  0.0000]]]) [['_ROOT_CHAR'], ['w', 'h', 'a', 't', 's'], ['u', 'p'], ['m', 'y'], ['f', 'r', 'i', 'e', 'n', 'd'], ['a', 'r', 'e'], ['y', 'o', 'u'], ['a', 'b', 'l', 'e'], ['t', 'o'], ['l', 'e', 'a', 'r', 'n'], ['s', 'o', 'm', 'e', 't', 'h', 'i', 'n', 'g'], ['_END']] ['_ROOT_CHAR', 'who the the the th', '_ROOT_CHAR', 'm an the the the t', 'lot the the the th', 'atin to so the the', 'yout the the the t', 'anoo the the the t', 'itn the the the th', 'lan the the the th', 'sooo the the the t', '_END']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NORMALIZING : [('NORMED', '_ROOT_CHAR'), ('NEED_NORM', 'whats'), ('NEED_NORM', 'up'), ('NEED_NORM', 'my'), ('NEED_NORM', 'friend'), ('NEED_NORM', 'are'), ('NEED_NORM', 'you'), ('NEED_NORM', 'able'), ('NEED_NORM', 'to'), ('NEED_NORM', 'learn'), ('NEED_NORM', 'something'), ('NORMED', '_END')] \n",
      "DECODED text is : ['_ROOT_CHAR', 'who the the the th', '_ROOT_CHAR', 'm an the the the t', 'lot the the the th', 'atin to so the the', 'yout the the the t', 'anoo the the the t', 'itn the the the th', 'lan the the the th', 'sooo the the the t', '_END'] original is ['_ROOT_CHAR', 'whats', 'up', 'my', 'friend', 'are', 'you', 'able', 'to', 'learn', 'something', '_END'] and None seen as word embed \n",
      "Please type what you want to normalize word by word and finishes by 'stop' ? to end type : 'END'    whats up my friend are you able to learn something\n",
      "Please type what you want to normalize word by word and finishes by 'stop' ? to end type : 'END'    stop\n",
      "sent ['whats up my friend are you able to learn something']\n",
      "Attention shape torch.Size([3, 18, 20])\n",
      "Attention tensor([[[ 0.4762,  0.2877,  0.2361,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0018,  0.0000,  0.9982,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0001,  0.9999,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         ...,\n",
      "         [ 0.0000,  0.0276,  0.9724,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0002,  0.8701,  0.1297,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.8732,  0.0074,  0.1194,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "        [[ 0.0486,  0.0296,  0.0387,  ...,  0.0539,  0.0375,  0.0750],\n",
      "         [ 0.0778,  0.0005,  0.0004,  ...,  0.0022,  0.0004,  0.0003],\n",
      "         [ 0.0011,  0.0012,  0.0022,  ...,  0.0010,  0.0021,  0.0015],\n",
      "         ...,\n",
      "         [ 0.0000,  0.0000,  0.0001,  ...,  0.0000,  0.0001,  0.0001],\n",
      "         [ 0.0000,  0.0038,  0.0039,  ...,  0.0002,  0.0031,  0.0195],\n",
      "         [ 0.0001,  0.0033,  0.0034,  ...,  0.0006,  0.0054,  0.0172]],\n",
      "\n",
      "        [[ 0.4750,  0.2892,  0.2358,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0043,  0.0000,  0.9957,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0002,  0.9998,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         ...,\n",
      "         [ 0.6419,  0.3355,  0.0227,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0001,  0.9046,  0.0953,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.8757,  0.0356,  0.0887,  ...,  0.0000,  0.0000,  0.0000]]]) [['_ROOT_CHAR'], ['w', 'h', 'a', 't', 's', ' ', 'u', 'p', ' ', 'm', 'y', ' ', 'f', 'r', 'i', 'e', 'n', 'd'], ['_END']] ['_ROOT_CHAR', 'whate and my and m', '_END']\n",
      "NORMALIZING : [('NORMED', '_ROOT_CHAR'), ('NEED_NORM', 'whats up my friend'), ('NORMED', '_END')] \n",
      "DECODED text is : ['_ROOT_CHAR', 'whate and my and m', '_END'] original is ['_ROOT_CHAR', 'whats up my friend', '_END'] and None seen as word embed \n",
      "Please type what you want to normalize word by word and finishes by 'stop' ? to end type : 'END'    do\n",
      "Please type what you want to normalize word by word and finishes by 'stop' ? to end type : 'END'    stop\n",
      "sent ['do']\n",
      "Attention shape torch.Size([3, 18, 4])\n",
      "Attention tensor([[[ 0.4762,  0.2877,  0.2361,  0.0000],\n",
      "         [ 0.0018,  0.0000,  0.9982,  0.0000],\n",
      "         [ 0.0000,  0.0001,  0.9999,  0.0000],\n",
      "         [ 0.0016,  0.1893,  0.8091,  0.0000],\n",
      "         [ 0.8973,  0.0772,  0.0255,  0.0000],\n",
      "         [ 0.0006,  0.8755,  0.1239,  0.0000],\n",
      "         [ 0.8219,  0.0326,  0.1455,  0.0000],\n",
      "         [ 0.0016,  0.7951,  0.2033,  0.0000],\n",
      "         [ 0.8249,  0.0015,  0.1736,  0.0000],\n",
      "         [ 0.0011,  0.0021,  0.9968,  0.0000],\n",
      "         [ 0.0001,  0.0508,  0.9492,  0.0000],\n",
      "         [ 0.0000,  0.0017,  0.9983,  0.0000],\n",
      "         [ 0.4682,  0.0345,  0.4973,  0.0000],\n",
      "         [ 0.0114,  0.0197,  0.9689,  0.0000],\n",
      "         [ 0.8124,  0.0259,  0.1617,  0.0000],\n",
      "         [ 0.0000,  0.0276,  0.9724,  0.0000],\n",
      "         [ 0.0002,  0.8701,  0.1297,  0.0000],\n",
      "         [ 0.8732,  0.0074,  0.1194,  0.0000]],\n",
      "\n",
      "        [[ 0.1562,  0.0730,  0.1921,  0.5786],\n",
      "         [ 0.0504,  0.0012,  0.5851,  0.3632],\n",
      "         [ 0.0029,  0.0019,  0.0017,  0.9935],\n",
      "         [ 0.0002,  0.0039,  0.0002,  0.9957],\n",
      "         [ 0.0002,  0.0008,  0.0005,  0.9985],\n",
      "         [ 0.0025,  0.0936,  0.0022,  0.9018],\n",
      "         [ 0.0000,  0.0000,  0.0000,  1.0000],\n",
      "         [ 0.0007,  0.0136,  0.0017,  0.9841],\n",
      "         [ 0.0000,  0.0000,  0.0000,  1.0000],\n",
      "         [ 0.0945,  0.0130,  0.4063,  0.4861],\n",
      "         [ 0.0696,  0.0421,  0.4496,  0.4387],\n",
      "         [ 0.0000,  0.0003,  0.0001,  0.9996],\n",
      "         [ 0.0000,  0.0005,  0.0000,  0.9995],\n",
      "         [ 0.1169,  0.0146,  0.5147,  0.3538],\n",
      "         [ 0.0627,  0.0292,  0.5180,  0.3902],\n",
      "         [ 0.0000,  0.0004,  0.0001,  0.9995],\n",
      "         [ 0.0000,  0.0006,  0.0000,  0.9994],\n",
      "         [ 0.1174,  0.0147,  0.5268,  0.3411]],\n",
      "\n",
      "        [[ 0.4750,  0.2892,  0.2358,  0.0000],\n",
      "         [ 0.0043,  0.0000,  0.9957,  0.0000],\n",
      "         [ 0.0000,  0.0002,  0.9998,  0.0000],\n",
      "         [ 0.0020,  0.2171,  0.7809,  0.0000],\n",
      "         [ 0.0097,  0.6353,  0.3550,  0.0000],\n",
      "         [ 0.5993,  0.3741,  0.0266,  0.0000],\n",
      "         [ 0.0122,  0.2722,  0.7156,  0.0000],\n",
      "         [ 0.8547,  0.0051,  0.1402,  0.0000],\n",
      "         [ 0.0000,  0.9903,  0.0097,  0.0000],\n",
      "         [ 0.0033,  0.8321,  0.1645,  0.0000],\n",
      "         [ 0.8490,  0.0239,  0.1271,  0.0000],\n",
      "         [ 0.0209,  0.1751,  0.8040,  0.0000],\n",
      "         [ 0.0000,  0.9999,  0.0001,  0.0000],\n",
      "         [ 0.0049,  0.6332,  0.3619,  0.0000],\n",
      "         [ 0.0000,  0.9986,  0.0014,  0.0000],\n",
      "         [ 0.6419,  0.3355,  0.0227,  0.0000],\n",
      "         [ 0.0001,  0.9046,  0.0953,  0.0000],\n",
      "         [ 0.8757,  0.0356,  0.0887,  0.0000]]]) [['_ROOT_CHAR'], ['d', 'o'], ['_END']] ['_ROOT_CHAR', 'werinan the the th', '_END']\n",
      "NORMALIZING : [('NORMED', '_ROOT_CHAR'), ('NEED_NORM', 'do'), ('NORMED', '_END')] \n",
      "DECODED text is : ['_ROOT_CHAR', 'werinan the the th', '_END'] original is ['_ROOT_CHAR', 'do', '_END'] and None seen as word embed \n",
      "Please type what you want to normalize word by word and finishes by 'stop' ? to end type : 'END'    do you understand what i'm saying\n",
      "Please type what you want to normalize word by word and finishes by 'stop' ? to end type : 'END'    stop\n",
      "sent [\"do you understand what i'm saying\"]\n",
      "Attention shape torch.Size([3, 18, 20])\n",
      "Attention tensor([[[ 0.4762,  0.2877,  0.2361,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0018,  0.0000,  0.9982,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0001,  0.9999,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         ...,\n",
      "         [ 0.0000,  0.0276,  0.9724,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0002,  0.8701,  0.1297,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.8732,  0.0074,  0.1194,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "        [[ 0.0473,  0.0249,  0.0565,  ...,  0.0514,  0.0812,  0.0555],\n",
      "         [ 0.0174,  0.0010,  0.3214,  ...,  0.0017,  0.0003,  0.0003],\n",
      "         [ 0.0294,  0.0181,  0.0311,  ...,  0.1021,  0.1553,  0.0658],\n",
      "         ...,\n",
      "         [ 0.0000,  0.0006,  0.0001,  ...,  0.0043,  0.4278,  0.0952],\n",
      "         [ 0.0016,  0.0070,  0.0029,  ...,  0.0208,  0.0598,  0.6603],\n",
      "         [ 0.0000,  0.0001,  0.0000,  ...,  0.0011,  0.3525,  0.0259]],\n",
      "\n",
      "        [[ 0.4750,  0.2892,  0.2358,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0043,  0.0000,  0.9957,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0002,  0.9998,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         ...,\n",
      "         [ 0.6419,  0.3355,  0.0227,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0001,  0.9046,  0.0953,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.8757,  0.0356,  0.0887,  ...,  0.0000,  0.0000,  0.0000]]]) [['_ROOT_CHAR'], ['d', 'o', ' ', 'y', 'o', 'u', ' ', 'u', 'n', 'd', 'e', 'r', 's', 't', 'a', 'n', 'd', ' '], ['_END']] ['_ROOT_CHAR', 'wee att me gooke m', '_END']\n",
      "NORMALIZING : [('NORMED', '_ROOT_CHAR'), ('NEED_NORM', 'do you understand '), ('NORMED', '_END')] \n",
      "DECODED text is : ['_ROOT_CHAR', 'wee att me gooke m', '_END'] original is ['_ROOT_CHAR', 'do you understand ', '_END'] and None seen as word embed \n",
      "Please type what you want to normalize word by word and finishes by 'stop' ? to end type : 'END'    you don't understand anything\n",
      "Please type what you want to normalize word by word and finishes by 'stop' ? to end type : 'END'    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please type what you want to normalize word by word and finishes by 'stop' ? to end type : 'END'    stop\n",
      "sent [\"you don't understand anything\"]\n",
      "Attention shape torch.Size([3, 18, 20])\n",
      "Attention tensor([[[ 0.4762,  0.2877,  0.2361,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0018,  0.0000,  0.9982,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0001,  0.9999,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         ...,\n",
      "         [ 0.0000,  0.0276,  0.9724,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0002,  0.8701,  0.1297,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.8732,  0.0074,  0.1194,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "        [[ 0.0465,  0.0425,  0.0478,  ...,  0.0453,  0.0468,  0.0811],\n",
      "         [ 0.0027,  0.0106,  0.1316,  ...,  0.0378,  0.0585,  0.0128],\n",
      "         [ 0.0001,  0.0006,  0.0013,  ...,  0.0006,  0.0007,  0.0015],\n",
      "         ...,\n",
      "         [ 0.0044,  0.0018,  0.0001,  ...,  0.0004,  0.0003,  0.0170],\n",
      "         [ 0.0059,  0.0023,  0.0002,  ...,  0.0005,  0.0003,  0.0208],\n",
      "         [ 0.0070,  0.0026,  0.0002,  ...,  0.0005,  0.0004,  0.0254]],\n",
      "\n",
      "        [[ 0.4750,  0.2892,  0.2358,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0043,  0.0000,  0.9957,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0002,  0.9998,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         ...,\n",
      "         [ 0.6419,  0.3355,  0.0227,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0001,  0.9046,  0.0953,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.8757,  0.0356,  0.0887,  ...,  0.0000,  0.0000,  0.0000]]]) [['_ROOT_CHAR'], ['y', 'o', 'u', ' ', 'd', 'o', 'n', \"'\", 't', ' ', 'u', 'n', 'd', 'e', 'r', 's', 't', 'a'], ['_END']] ['_ROOT_CHAR', 'you beeeeeeeeeeeep', '_END']\n",
      "NORMALIZING : [('NORMED', '_ROOT_CHAR'), ('NEED_NORM', \"you don't understa\"), ('NORMED', '_END')] \n",
      "DECODED text is : ['_ROOT_CHAR', 'you beeeeeeeeeeeep', '_END'] original is ['_ROOT_CHAR', \"you don't understa\", '_END'] and None seen as word embed \n"
     ]
    }
   ],
   "source": [
    "for folder_name in list_:\n",
    "    assert len(list_)>0, \"list empty\"\n",
    "    model_full_name = folder_name[:-7]\n",
    "    print(\"\\n\\n--------------------------------Interatcing with new model--------------------------------\\n\", model_full_name,\"\\n\\n\")\n",
    "    dic_path = os.path.join(script_dir, \"..\", \"checkpoints\", model_full_name + \"-folder\", \"dictionaries\")\n",
    "    model_dir = os.path.join(script_dir, \"..\", \"checkpoints\", model_full_name + \"-folder\")\n",
    "    interact(dic_path=dic_path, dir_model=model_dir, model_full_name=model_full_name, \n",
    "             debug=False, \n",
    "             show_attention=False, save_attention=False,\n",
    "             extra_arg_specific_label=\"\",\n",
    "             verbose=0)\n",
    "    #break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comments\n",
    "- on model 97077_rioc-ee386-REP_-replicate1-1dir-scale_1-model_15_f19e with auxilliary task no attention quite small model\n",
    "    - interesting case : \n",
    "        - my normalized to tomorrow : probably bcause of 2mor --> tommorow \n",
    "    - very lame at decoding very short word ? \n",
    "- 97147_rioc-6ed16-ATTCONTEXT-with_att-dir_2-X-dropout_bridge0.1-context_word-model_4_76a1\n",
    "    - attention is not static --> but no clear trend\n",
    "    - interetsintgly with same model but only using sent context : the attention focus on last word all the time \"97147_rioc-6ed16-ATTCONTEXT-with_att-dir_2-X-dropout_bridge0.1-context_sent-model_2_7d92\" \n",
    "    \n",
    "- \"97184_rioc-0cc46-ATTstandart_passing-with_att-dir_2-X-context_all-stable_decoding_state-init_context_decoder_False-model_4_2ffa\"\n",
    "    - the best model you played with so far\n",
    "    - classification does not agree with seq predictiion : most of the classification is NORMED while it performs some change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage: open [-e] [-t] [-f] [-W] [-R] [-n] [-g] [-h] [-s <partial SDK name>][-b <bundle identifier>] [-a <application>] [filenames] [--args arguments]\r\n",
      "Help: Open opens files from a shell.\r\n",
      "      By default, opens each file using the default application for that file.  \r\n",
      "      If the file is in the form of a URL, the file will be opened as a URL.\r\n",
      "Options: \r\n",
      "      -a                Opens with the specified application.\r\n",
      "      -b                Opens with the specified application bundle identifier.\r\n",
      "      -e                Opens with TextEdit.\r\n",
      "      -t                Opens with default text editor.\r\n",
      "      -f                Reads input from standard input and opens with TextEdit.\r\n",
      "      -F  --fresh       Launches the app fresh, that is, without restoring windows. Saved persistent state is lost, excluding Untitled documents.\r\n",
      "      -R, --reveal      Selects in the Finder instead of opening.\r\n",
      "      -W, --wait-apps   Blocks until the used applications are closed (even if they were already running).\r\n",
      "          --args        All remaining arguments are passed in argv to the application's main() function instead of opened.\r\n",
      "      -n, --new         Open a new instance of the application even if one is already running.\r\n",
      "      -j, --hide        Launches the app hidden.\r\n",
      "      -g, --background  Does not bring the application to the foreground.\r\n",
      "      -h, --header      Searches header file locations for headers matching the given filenames, and opens them.\r\n",
      "      -s                For -h, the SDK to use; if supplied, only SDKs whose names contain the argument value are searched.\r\n",
      "                        Otherwise the highest versioned SDK in each platform is used.\r\n"
     ]
    }
   ],
   "source": [
    "!open"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"./dat"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
