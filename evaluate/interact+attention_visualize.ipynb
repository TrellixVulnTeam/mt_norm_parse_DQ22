{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline  \n",
    "import sys\n",
    "sys.path.insert(0,\"..\")\n",
    "from model.sequence_prediction import greedy_decode_batch, decode_seq_str, decode_interacively\n",
    "import pdb\n",
    "from model.loss import LossCompute\n",
    "import os\n",
    "from io_.info_print import printing\n",
    "from model.seq2seq import LexNormalizer\n",
    "from model.generator import Generator\n",
    "from evaluate.interact import interact\n",
    "MAX_LEN = 20\n",
    "script_dir = \".\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_folder_starts_with = \"8ce6b-extend_ep-get_True-attention_simplifiedXauxXdropout0.1_scale_aux-True_aux-0.1do_char_dec-True_char_src_atten-model_14_ad6c\"\n",
    "#model_folder_starts_with = \"a5c77\"\n",
    "model_folder_starts_with =\"8ce6b\"\n",
    "model_folder_starts_with = \"f2f2-batchXdropout_char0.1-to_char_src-1_dir_sent-10_batch_size-model_18_aa04\"\n",
    "model_folder_starts_with = \"8e628\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list all models folder that starts with model_folder_starts_with\n",
    "list_all_dir = os.listdir(\"../checkpoints/\")\n",
    "list_ = [dir_ for dir_ in list_all_dir if dir_.startswith(model_folder_starts_with) and not dir_.endswith(\"log\") and not dir_.endswith(\"summary\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model to interact with \n",
    "#### Former models\n",
    "- f2f2-batchXdropout_char0.1-to_char_src-1_dir_sent-10_batch_size-model_18_aa04 : very bad at interacting\n",
    "\n",
    "- attention ; no aux (no bucket , get_batch False ) : 8e628 - attention degrades abit the results (still feedin char embedding also)\n",
    "\n",
    "\n",
    "#### Last two ablation with extending epoch + mix data + ablation on auxilliary task ponderation + attention or not  \n",
    "\n",
    "- 8ce6b-extend_ep... \n",
    "    - super lame attention model (from ablation with all context)\n",
    "    - the other one a bit better   \n",
    "- a5c77 a bit better but still : no attention much better : cf. plot \n",
    "\n",
    "NB : attention makes training 10 times slower  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!open //Users/bemuller/Documents/Work/INRIA/dev/mt_norm_parse/reports/extend_ep-SENT_context-get_True-attention_simplifiedXauxXdropout-last+bucket_False_eval-get_batch_False.html\n",
    "#!open //Users/bemuller/Documents/Work/INRIA/dev/mt_norm_parse/reports/8ce6b-extend_ep-get_True-attention_simplifiedXauxXdropout.html\n",
    "#!open /Users/bemuller/Documents/Work/INRIA/dev/mt_norm_parse/reports/8e628-no_bucketing-get_batch_False-train-attention-last+bucket_False_eval-get_batch_False-report.json.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "--------------------------------Interatcing with new model--------------------------------\n",
      " 8e628-no_bucketing-get_batch_False-train-attentionNone-dense_dim_auxilliary0weight_binary_loss0.2-to_char_src-1_dir_sent-10_batch-dir_word_src_1-unrolling_word_True-char_src_attention_False-model_1_85ab \n",
      "\n",
      "\n",
      "Loading dictionary from ./../checkpoints/8e628-no_bucketing-get_batch_False-train-attentionNone-dense_dim_auxilliary0weight_binary_loss0.2-to_char_src-1_dir_sent-10_batch-dir_word_src_1-unrolling_word_True-char_src_attention_False-model_1_85ab-folder/dictionaries \n",
      "Character vocabulary is 109 length\n",
      "WARNING : checkpoint_dir as indicated in args.json is not found, lt checkpoint_dir /\n",
      "Loading model with argument {'hidden_size_decoder': 50, 'encoder_arch': {'drop_out_word_encoder_out': 0.2, 'drop_out_sent_encoder_out': 0.2, 'dir_word': 'uni', 'dropout_sent_encoder_cell': 0.0, 'cell_word': 'LSTM', 'dropout_word_encoder_cell': 0.0, 'dir_word_encoder': 1, 'cell_sentence': 'LSTM', 'n_layers_word_encoder': 1, 'dir_sent_encoder': 1, 'attention': 'No'}, 'hidden_size_sent_encoder': 50, 'char_embedding_dim': 50, 'n_trainable_parameters': 87809, 'auxilliary_arch': {'weight_binary_loss': 0, 'auxilliary_task_norm_not_norm': False, 'auxilliary_task_norm_not_norm-dense_dim': None}, 'voc_size': 109, 'output_dim': 100, 'decoder_arch': {'unrolling_word': True, 'dir_word': 'uni', 'cell_word': 'LSTM', 'drop_out_word_decoder_cell': 0.0, 'drop_out_char_embedding_decoder': 0.1, 'char_src_attention': False, 'cell_sentence': 'none', 'attention': 'No'}, 'hidden_size_encoder': 50}\n",
      "Model arguments are {'info_checkpoint': {'other': {'weight_binary_loss': 0, 'error_curves_details': None, 'loss': 0.0014323253487527172, 'seed(np/torch)': [123, 123], 'error_curves': '/scratch/bemuller/mt_norm_parse/env/../checkpoints/8e628-no_bucketing-get_batch_False-train-attentionNone-dense_dim_auxilliary0weight_binary_loss0.2-to_char_src-1_dir_sent-10_batch-dir_word_src_1-unrolling_word_True-char_src_attention_False-model_1_85ab-folder/8e628-no_bucketing-get_batch_False-train-attentionNone-dense_dim_auxilliary0weight_binary_loss0.2-to_char_src-1_dir_sent-10_batch-dir_word_src_1-unrolling_word_True-char_src_attention_False-model_1_85ab-last-plo-seq.png', 'data': 'dev', 'time_training(min)': '113.63', 'average_per_epoch(min)': '1.14'}, 'dev_data_path': '/scratch/bemuller/mt_norm_parse/env/../../parsing/normpar/data/owoputi.integrated_fixed', 'n_epochs': 84, 'git_id': '563aafa8346f2e57c88a07ffc47b383b30c02b63', 'batch_size': 10, 'train_data_path': '/scratch/bemuller/mt_norm_parse/env/.././data/LiLiu/2577_tweets-li.conll'}, 'checkpoint_dir': '/scratch/bemuller/mt_norm_parse/env/../checkpoints/8e628-no_bucketing-get_batch_False-train-attentionNone-dense_dim_auxilliary0weight_binary_loss0.2-to_char_src-1_dir_sent-10_batch-dir_word_src_1-unrolling_word_True-char_src_attention_False-model_1_85ab-folder/8e628-no_bucketing-get_batch_False-train-attentionNone-dense_dim_auxilliary0weight_binary_loss0.2-to_char_src-1_dir_sent-10_batch-dir_word_src_1-unrolling_word_True-char_src_attention_False-model_1_85ab-Xep-outof100ep-checkpoint.pt', 'hyperparameters': {'hidden_size_decoder': 50, 'encoder_arch': {'drop_out_word_encoder_out': 0.2, 'drop_out_sent_encoder_out': 0.2, 'dir_word': 'uni', 'dropout_sent_encoder_cell': 0.0, 'cell_word': 'LSTM', 'dropout_word_encoder_cell': 0.0, 'dir_word_encoder': 1, 'cell_sentence': 'LSTM', 'n_layers_word_encoder': 1, 'dir_sent_encoder': 1, 'attention': 'No'}, 'hidden_size_sent_encoder': 50, 'char_embedding_dim': 50, 'n_trainable_parameters': 87809, 'auxilliary_arch': {'weight_binary_loss': 0, 'auxilliary_task_norm_not_norm': False, 'auxilliary_task_norm_not_norm-dense_dim': None}, 'voc_size': 109, 'output_dim': 100, 'decoder_arch': {'unrolling_word': True, 'dir_word': 'uni', 'cell_word': 'LSTM', 'drop_out_word_decoder_cell': 0.0, 'drop_out_char_embedding_decoder': 0.1, 'char_src_attention': False, 'cell_sentence': 'none', 'attention': 'No'}, 'hidden_size_encoder': 50}} \n",
      "WARNING : hidden_size of word_recurrent_cell has been divided by 1 dir_word_encoder\n",
      "WARNING : DECODER unrolling_word is True\n",
      "WARNING : DECODER char_src_attention is False\n",
      "INFO : dictionary is None so setting char_dictionary to model.char_dictionary\n",
      "Please type what you want to normalize word by word and finishes by 'stop' ? to end type : 'END'    haha\n",
      "Please type what you want to normalize word by word and finishes by 'stop' ? to end type : 'END'    okayy\n",
      "Please type what you want to normalize word by word and finishes by 'stop' ? to end type : 'END'    lets\n",
      "Please type what you want to normalize word by word and finishes by 'stop' ? to end type : 'END'    do\n",
      "Please type what you want to normalize word by word and finishes by 'stop' ? to end type : 'END'    itt\n",
      "Please type what you want to normalize word by word and finishes by 'stop' ? to end type : 'END'    stop\n",
      "sent ['haha', 'okayy', 'lets', 'do', 'itt', '']\n",
      "WARNING : we added _START symbol and _END_CHAR ! \n",
      "WARNING : we added _START symbol and _END_CHAR ! \n",
      "WARNING : we added _START symbol and _END_CHAR ! \n",
      "WARNING : we added _START symbol and _END_CHAR ! \n",
      "WARNING : we added _START symbol and _END_CHAR ! \n",
      "DECODED text is : ['hahaha', 'okaydayly', 'letsess', 'do', 'ittal'] original is ['haha', 'okayy', 'lets', 'do', 'itt']\n"
     ]
    }
   ],
   "source": [
    "for folder_name in list_:\n",
    "    model_full_name = folder_name[:-7]\n",
    "    print(\"\\n\\n--------------------------------Interatcing with new model--------------------------------\\n\", model_full_name,\"\\n\\n\")\n",
    "    dic_path = os.path.join(script_dir, \"..\", \"checkpoints\", model_full_name + \"-folder\", \"dictionaries\")\n",
    "    model_dir = os.path.join(script_dir, \"..\", \"checkpoints\", model_full_name + \"-folder\")\n",
    "    interact(dic_path=dic_path, dir_model=model_dir, model_full_name=model_full_name, debug=False, \n",
    "             show_attention=False,verbose=0,\n",
    "             save_attention=False)\n",
    "    #break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
